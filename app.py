import streamlit as st
from pinecone import Pinecone
import google.generativeai as genai
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pypdf import PdfReader
from docx import Document
import time

# --- C·∫§U H√åNH ---
# L·∫•y t·ª´ Secrets (tr√™n Cloud) ho·∫∑c ƒëi·ªÅn tr·ª±c ti·∫øp n·∫øu ch·∫°y local
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    PINECONE_API_KEY = st.secrets["PINECONE_API_KEY"]
    ADMIN_PASSWORD = st.secrets.get("ADMIN_PASSWORD", "123456") # M·∫≠t kh·∫©u m·∫∑c ƒë·ªãnh l√† 123456
except:
    # Fallback cho ch·∫°y local n·∫øu ch∆∞a setup secrets
    GOOGLE_API_KEY = "ƒêI·ªÄN_KEY_GOOGLE_CUA_BAN"
    PINECONE_API_KEY = "ƒêI·ªÄN_KEY_PINECONE_CUA_BAN"
    ADMIN_PASSWORD = "123456" 

PINECONE_INDEX_NAME = "chatbot-demo"

# Setup
genai.configure(api_key=GOOGLE_API_KEY)
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(PINECONE_INDEX_NAME)

st.set_page_config(page_title="Chatbot T√†i Li·ªáu", page_icon="ü§ñ", layout="wide")

# --- PH·∫¶N 1: ADMIN PANEL (N·∫†P D·ªÆ LI·ªÜU) ---
with st.sidebar:
    st.header("‚öôÔ∏è Qu·∫£n tr·ªã vi√™n")
    password = st.text_input("Nh·∫≠p m·∫≠t kh·∫©u Admin", type="password")
    
    if password == ADMIN_PASSWORD:
        st.success("ƒê√£ m·ªü kh√≥a t√≠nh nƒÉng n·∫°p d·ªØ li·ªáu!")
        uploaded_files = st.file_uploader("Upload t√†i li·ªáu (PDF, DOCX, TXT)", accept_multiple_files=True)
        
        if st.button("X·ª≠ l√Ω & N·∫°p v√†o AI"):
            if not uploaded_files:
                st.warning("Vui l√≤ng ch·ªçn file tr∆∞·ªõc!")
            else:
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
                vectors_to_upsert = []
                total_files = len(uploaded_files)
                
                for i, file in enumerate(uploaded_files):
                    status_text.text(f"ƒêang ƒë·ªçc file: {file.name}...")
                    
                    # ƒê·ªçc n·ªôi dung file
                    text = ""
                    try:
                        if file.name.endswith('.pdf'):
                            pdf = PdfReader(file)
                            for page in pdf.pages: text += page.extract_text() or ""
                        elif file.name.endswith('.docx'):
                            doc = Document(file)
                            for para in doc.paragraphs: text += para.text + "\n"
                        elif file.name.endswith('.txt'):
                            text = file.read().decode("utf-8")
                    except Exception as e:
                        st.error(f"L·ªói ƒë·ªçc file {file.name}: {e}")
                        continue
                        
                    # Chia nh·ªè & Embedding
                    chunks = text_splitter.split_text(text)
                    for chunk_id, chunk_text in enumerate(chunks):
                        try:
                            embedding = genai.embed_content(
                                model="models/text-embedding-004",
                                content=chunk_text,
                                task_type="retrieval_document"
                            )['embedding']
                            
                            vector_id = f"{file.name}_{chunk_id}"
                            metadata = {"text": chunk_text, "source": file.name}
                            vectors_to_upsert.append((vector_id, embedding, metadata))
                        except Exception as e:
                            pass # B·ªè qua l·ªói nh·ªè ƒë·ªÉ ch·∫°y ti·∫øp
                    
                    # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh
                    progress_bar.progress((i + 1) / total_files)

                # ƒê·∫©y l√™n Pinecone
                status_text.text("ƒêang ƒë·∫©y d·ªØ li·ªáu l√™n Cloud...")
                batch_size = 50
                for i in range(0, len(vectors_to_upsert), batch_size):
                    batch = vectors_to_upsert[i:i+batch_size]
                    index.upsert(vectors=batch)
                    time.sleep(1) # Tr√°nh rate limit
                
                status_text.text("‚úÖ Ho√†n t·∫•t! D·ªØ li·ªáu m·ªõi ƒë√£ s·∫µn s√†ng.")
                st.balloons()
    elif password:
        st.error("Sai m·∫≠t kh·∫©u!")

# --- PH·∫¶N 2: GIAO DI·ªÜN CHAT (CHO NG∆Ø·ªúI D√ôNG) ---
st.title("ü§ñ Tr·ª£ l√Ω Tra C·ª©u T√†i Li·ªáu")
st.caption("H·ªèi ƒë√°p mi·ªÖn ph√≠ d·ª±a tr√™n 500 t√†i li·ªáu ƒë√£ cung c·∫•p.")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

def get_relevant_context(query):
    try:
        query_embedding = genai.embed_content(model="models/text-embedding-004", content=query, task_type="retrieval_query")['embedding']
        results = index.query(vector=query_embedding, top_k=5, include_metadata=True)
        context_text = ""
        for match in results['matches']:
            context_text += f"\n[Ngu·ªìn: {match['metadata'].get('source', 'Unknown')}]: {match['metadata'].get('text', '')}\n---\n"
        return context_text
    except:
        return ""

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang t√¨m ki·∫øm..."):
            context = get_relevant_context(prompt)
            if not context:
                response_text = "T√¥i ch∆∞a c√≥ d·ªØ li·ªáu v·ªÅ v·∫•n ƒë·ªÅ n√†y, ho·∫∑c h·ªá th·ªëng d·ªØ li·ªáu ƒëang tr·ªëng."
            else:
                full_prompt = f"Th√¥ng tin: {context}\nC√¢u h·ªèi: {prompt}\nH√£y tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tr√™n."
                try:
                    model = genai.GenerativeModel('gemini-1.5-flash')
                    response = model.generate_content(full_prompt)
                    response_text = response.text
                except Exception as e:
                    response_text = f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {e}"
            
            st.markdown(response_text)
            st.session_state.messages.append({"role": "assistant", "content": response_text})